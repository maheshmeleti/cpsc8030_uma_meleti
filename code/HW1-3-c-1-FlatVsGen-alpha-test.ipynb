{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9BBPiirv_Kh8"},"outputs":[],"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import sklearn\n","import seaborn\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import optim\n","import torch\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","from torchvision import transforms\n","\n","from torch.utils.data import TensorDataset\n","from tqdm import tqdm\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.utils.data import DataLoader\n","\n","import matplotlib.pyplot as plt\n","import random\n","from sklearn.decomposition import PCA"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"q9aeQ1i9H8QI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Trainer:\n","\n","    def __init__(self,\n","                 model: torch.nn.Module,\n","                 device: torch.device,\n","                 criterion: torch.nn.Module,\n","                 optimizer: torch.optim.Optimizer,\n","                 training_DataLoader: torch.utils.data.Dataset,\n","                 validation_DataLoader: None,\n","                 # lr_scheduler: torch.optim.lr_scheduler = None,\n","                 epochs: int = 100,\n","                 epoch: int = 0,\n","                 notebook: bool = False,\n","                 path2write: str = None,\n","                 save_best=False,\n","                 save_final=True,\n","                 save_interval=10,\n","                 checkpoint_start_epoch=50\n","                 ):\n","        self.model = model\n","        self.criterion = criterion\n","        self.optimizer = optimizer\n","        # self.lr_scheduler = lr_scheduler\n","        self.training_DataLoader = training_DataLoader\n","        self.validation_DataLoader = validation_DataLoader\n","        self.device = device\n","        self.epochs = epochs\n","        self.epoch = epoch\n","        self.notebook = notebook\n","        self.path2write = path2write\n","        LOG_DIR = os.path.join(path2write, 'Log')  # path2write + 'Log/'\n","        self.writer_train = SummaryWriter(os.path.join(LOG_DIR, \"train\"))\n","        self.writer_val = SummaryWriter(os.path.join(LOG_DIR, \"val\"))\n","        self.check_point_path = os.path.join(path2write, 'check_points')\n","        if not os.path.exists(self.check_point_path):\n","            os.makedirs(self.check_point_path)\n","        self.save_best = save_best\n","        self.save_final = save_final\n","        self.save_interval = save_interval\n","        self.checkpoint_start_epoch = checkpoint_start_epoch\n","        self.training_loss = []\n","        self.validation_loss = []\n","        self.learning_rate = []\n","        self.training_accuracy = []\n","        self.validation_accuracy = []\n","\n","    def run_trainer(self):\n","        self.model.to(self.device)\n","        #         print(next(self.model.parameters()).device)\n","        if self.notebook:\n","            print('Notebook')\n","            from tqdm.notebook import tqdm, trange\n","        else:\n","            from tqdm import tqdm, trange\n","        #         print(self.epochs)\n","        progressbar = trange(self.epochs, desc='Progress', disable=True)  # don't show progressbar\n","        loss_max = None\n","        for epoch in progressbar:\n","            print(f'Epoch - {epoch}')\n","\n","            # Training Block\n","            train_loss, train_accuracy = self._train()\n","            self.writer_train.add_scalar(\"Train Loss\", train_loss, epoch)\n","            self.writer_train.add_scalar(\"Train Accuracy\", train_accuracy, epoch)\n","\n","\n","            # Val Block\n","            val_loss, val_accuracy = self._validate()\n","            self.writer_val.add_scalar(\"Val Loss\", val_loss, epoch)\n","            self.writer_val.add_scalar(\"Val Accuracy\", val_accuracy, epoch)\n","\n","            # lr\n","            self.writer_train.add_scalar(\"Learning Rate\", self.optimizer.param_groups[0]['lr'], epoch)\n","\n","            print('Epoch - {} Train Loss - {:.6f} Val Loss - {:.6f} Train Accuracy - {:.6f} Val Accuracy - {:.6f}'.format(epoch, train_loss, val_loss, train_accuracy, val_accuracy))\n","            if self.save_final:\n","                if epoch == self.epochs-1:\n","                    model_name = 'epoch-{}-loss{:.6f}'.format(epoch, val_loss)\n","                    torch.save(self.model.state_dict(), os.path.join(self.check_point_path, model_name))\n","            loss_max = val_loss\n","\n","        return self.training_loss, self.validation_loss, self.model, self.training_accuracy, self.validation_accuracy\n","\n","    def _train(self):\n","\n","        self.model.train()\n","        train_losses = []\n","        batch_iter = tqdm(enumerate(self.training_DataLoader), 'Training', total=len(self.training_DataLoader),\n","                          disable=False)\n","        batch_acc = 0\n","        for i, (x, y) in batch_iter:\n","            input, target = x.type(torch.float32).to(self.device), y.type(torch.float32).to(self.device)\n","            self.optimizer.zero_grad()\n","            target = target.type(torch.LongTensor).to(self.device)\n","            output = self.model(input)\n","            loss = self.criterion(output, target)\n","            train_losses.append(loss.item())\n","            loss.backward()\n","            self.optimizer.step()\n","            pred = output.argmax(dim=1, keepdim=True) # max of prob\n","            pred = pred.flatten()\n","            batch_acc += torch.mean(pred.eq(target.view_as(pred)).type(torch.FloatTensor))\n","        accuracy = batch_acc/len(self.training_DataLoader)\n","        self.training_loss.append(np.mean(train_losses))  # Mean batch loss\n","        self.learning_rate.append(self.optimizer.param_groups[0]['lr'])\n","        self.training_accuracy.append(accuracy)\n","\n","        batch_iter.close()  # clean up the bar\n","        return np.mean(train_losses), accuracy\n","\n","    def _validate(self):\n","\n","        self.model.eval()\n","        valid_losses = []\n","        batch_iter = tqdm(enumerate(self.validation_DataLoader), 'validation', total=len(self.validation_DataLoader), disable=False)\n","        batch_acc = 0\n","        for i, (x, y) in batch_iter:\n","            input, target = x.type(torch.float32).to(self.device), y.to(self.device)\n","            with torch.no_grad():\n","                output = self.model(input)\n","                target = target.type(torch.LongTensor).to(self.device)\n","                loss = self.criterion(output, target)\n","                valid_losses.append(loss.item())\n","                pred = output.argmax(dim=1, keepdim=True)\n","                batch_acc += torch.mean(pred.eq(target.view_as(pred)).type(torch.FloatTensor)).item()\n","\n","        accuracy = batch_acc/len(self.validation_DataLoader)\n","        self.validation_loss.append(np.mean(valid_losses))\n","        self.validation_accuracy.append(accuracy)\n","        batch_iter.close()\n","        return np.mean(valid_losses), accuracy"],"metadata":{"id":"pCrOa33yBdVF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DNN(nn.Module): #for mnist\n","  def __init__(self):\n","    super().__init__()\n","    self.Dense1 = nn.Linear(28*28, 64)\n","    self.Dense2 = nn.Linear(64, 32)\n","    self.Dense3 = nn.Linear(32, 16)\n","    self.Dense4 = nn.Linear(16, 10)\n","\n","  def forward(self, x):\n","    x = x.view(x.shape[0], -1)\n","    x = self.Dense1(x)\n","    x = self.Dense2(x)\n","    x = self.Dense3(x)\n","    x = self.Dense4(x)\n","    out = F.log_softmax(x)\n","\n","    return out"],"metadata":{"id":"zzAXg75IB1mk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gpu_id = 0\n","loss_fn = nn.CrossEntropyLoss()\n","epochs =  20\n","notebook = True\n","checkpoint_start_epoch = 5 #Not using\n","path2write = \"drive/MyDrive/DL_homework/HW1_1/\""],"metadata":{"id":"sk7RTkeADOqO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bath_size = 256\n","lr = 1e-4\n","\n","transform=transforms.Compose([\n","        transforms.ToTensor()\n","        ])\n","dataset1 = datasets.MNIST('../data', train=True, download=True, transform=transform)\n","dataset2 = datasets.MNIST('../data', train=False, transform=transform)\n","training_DataLoader = DataLoader(dataset1, batch_size=bath_size, shuffle=True)\n","validation_DataLoader= DataLoader(dataset2, batch_size=bath_size, shuffle=True)\n","\n","\n","model = DNN()\n","optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n","trainer = Trainer(model=model,\n","                        device=gpu_id,\n","                        criterion=loss_fn,\n","                        optimizer=optimizer,\n","                        training_DataLoader=training_DataLoader,\n","                        validation_DataLoader=validation_DataLoader,\n","                        # lr_scheduler=lr_scheduler,\n","                        epochs=epochs,\n","                        epoch=0,\n","                        notebook=True,\n","                        path2write= path2write,\n","                        checkpoint_start_epoch=checkpoint_start_epoch )\n","training_loss_lr1e4_B256, validation_loss_lr1e4_B256, model_lr1e4_B256, training_accuracy_lr1e4_B256, validation_accuracy_lr1e4_B256 = trainer.run_trainer()\n","\n"],"metadata":{"id":"uYOJnL9yDrAv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bath_size = 1024\n","lr = 1e-4\n","\n","transform=transforms.Compose([\n","        transforms.ToTensor()\n","        ])\n","dataset1 = datasets.MNIST('../data', train=True, download=True, transform=transform)\n","dataset2 = datasets.MNIST('../data', train=False, transform=transform)\n","training_DataLoader = DataLoader(dataset1, batch_size=bath_size, shuffle=True)\n","validation_DataLoader= DataLoader(dataset2, batch_size=bath_size, shuffle=True)\n","\n","\n","model = DNN()\n","optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n","trainer = Trainer(model=model,\n","                        device=gpu_id,\n","                        criterion=loss_fn,\n","                        optimizer=optimizer,\n","                        training_DataLoader=training_DataLoader,\n","                        validation_DataLoader=validation_DataLoader,\n","                        # lr_scheduler=lr_scheduler,\n","                        epochs=epochs,\n","                        epoch=0,\n","                        notebook=True,\n","                        path2write= path2write,\n","                        checkpoint_start_epoch=checkpoint_start_epoch )\n","training_loss_lr1e4_B1024, validation_loss_lr1e4_B1024, model_lr1e4_B1024, training_accuracy_lr1e4_B1024, validation_accuracy_lr1e4_B1024 = trainer.run_trainer()"],"metadata":{"id":"RqbK_EtzEXsV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bath_size = 1024\n","lr = 1e-2\n","\n","transform=transforms.Compose([\n","        transforms.ToTensor()\n","        ])\n","dataset1 = datasets.MNIST('../data', train=True, download=True, transform=transform)\n","dataset2 = datasets.MNIST('../data', train=False, transform=transform)\n","training_DataLoader = DataLoader(dataset1, batch_size=bath_size, shuffle=True)\n","validation_DataLoader= DataLoader(dataset2, batch_size=bath_size, shuffle=True)\n","\n","\n","model = DNN()\n","optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n","trainer = Trainer(model=model,\n","                        device=gpu_id,\n","                        criterion=loss_fn,\n","                        optimizer=optimizer,\n","                        training_DataLoader=training_DataLoader,\n","                        validation_DataLoader=validation_DataLoader,\n","                        # lr_scheduler=lr_scheduler,\n","                        epochs=epochs,\n","                        epoch=0,\n","                        notebook=True,\n","                        path2write= path2write,\n","                        checkpoint_start_epoch=checkpoint_start_epoch )\n","training_loss_lr1e2_B1024, validation_loss_lr1e2_B1024, model_lr1e2_B1024, training_accuracy_lr1e2_B1024, validation_accuracy_lr1e2_B1024 = trainer.run_trainer()"],"metadata":{"id":"Vpi3XE9mGh4t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = plt.figure(figsize=(15,5))\n","\n","ax1 = fig.add_subplot(1,2,1)\n","ax1.plot(training_loss_lr1e4_B256, color='orange', label='loss batch size - 256, lr=1e-4')\n","ax1.plot(training_loss_lr1e4_B1024, color='blue', label='loss batch size - 1024, lr=1e-4')\n","ax1.legend()\n","ax1.set_title('Batchsize Vs Loss')\n","ax1.set_xlabel('Epochs')\n","ax1.set_ylabel('Cross Entrophy Loss')\n","fig.savefig(os.path.join(path2write, 'FlatVSGen_loss_constantB.png'))\n","\n","ax2 = fig.add_subplot(1,2,2)\n","ax2.plot(training_loss_lr1e2_B1024, color='orange', label='loss batch size - 1024, lr=1e-2')\n","ax2.plot(training_loss_lr1e4_B1024, color='blue', label='loss batch size - 1024, lr=1e-4')\n","ax2.legend()\n","ax2.set_title('Learning Rate Vs Loss')\n","ax2.set_xlabel('Epochs')\n","ax2.set_ylabel('Cross Entrophy Loss')\n","fig.savefig(os.path.join(path2write, 'FlatVSGen_loss_constantLr.png'))\n","\n","plt.show()\n"],"metadata":{"id":"cn5tgU2bG2jj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# alpha test\n","def inference(model, DataLoader, criterion, device=0):\n","  model.eval()\n","  valid_losses = []\n","  batch_iter = tqdm(enumerate(DataLoader), 'inference_loader', total=len(DataLoader), disable=False)\n","  batch_acc = 0\n","  for i, (x, y) in batch_iter:\n","      input, target = x.type(torch.float32).to(device), y.to(device)\n","      with torch.no_grad():\n","          output = model(input)\n","          target = target.type(torch.LongTensor).to(device)\n","          loss = criterion(output, target)\n","          valid_losses.append(loss.item())\n","          pred = output.argmax(dim=1, keepdim=True)\n","          batch_acc += torch.mean(pred.eq(target.view_as(pred)).type(torch.FloatTensor)).item()\n","  batch_iter.close()\n","  accuracy = batch_acc/len(DataLoader)\n","  loss = np.mean(valid_losses)\n","  return accuracy, loss"],"metadata":{"id":"bqH6IX0UKzIT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def alpha_test(model, model1, model2, training_DataLoader, validation_DataLoader, loss_fn):\n","  alphas = np.linspace(-1, 1, 10)\n","  train_loss = []\n","  train_accuracy = []\n","  val_loss = []\n","  val_accuracy = []\n","  model1_ = model1.state_dict()\n","  model2_ = model2.state_dict()\n","  model.to(device=0)\n","  for i in range(len(alphas)):\n","    alpha = alphas[i]\n","    parm_ = {}\n","    for key in  model1_.keys():\n","      parm_[key] = (1-alpha)*model1_[key] + alpha*model2_[key]\n","    model.load_state_dict(parm_)\n","    train_accuracy_, train_loss_ = inference(model, training_DataLoader, loss_fn, device=0)\n","    val_accuracy_, val_loss_ = inference(model, validation_DataLoader, loss_fn, device=0)\n","    train_loss.append(train_loss_)\n","    train_accuracy.append(train_accuracy_)\n","    val_loss.append(val_loss_)\n","    val_accuracy.append(val_accuracy_)\n","  return alphas, train_loss, train_accuracy, val_loss, val_accuracy"],"metadata":{"id":"xpMBJ7E1VAqs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#lr constant\n","model = DNN()\n","model1 = model_lr1e4_B256\n","model2 = model_lr1e4_B1024\n","alphasB, train_lossB, train_accuracyB, val_lossB, val_accuracyB = alpha_test(model, model1, model2, training_DataLoader, validation_DataLoader, loss_fn)\n"],"metadata":{"id":"vv_Ngz2aPE91"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax1 = plt.subplots(figsize=(20, 5))\n","\n","ax1 = fig.add_subplot(1,1,1)\n","ax1.plot(alphasB, train_lossB, 'b-', label='Train')\n","ax1.plot(alphasB, val_lossB, 'b--', label='Validation')\n","ax1.legend()\n","ax1.set_xlabel('Alpha')\n","ax1.set_ylabel('Loss', color='b')\n","ax1.set_title('lr constant; Batch Size 256 Vs 1024')\n","\n","ax2 = ax1.twinx()\n","ax2.plot(alphasB, train_accuracyB, 'r-')\n","ax2.plot(alphasB, val_accuracyB, 'r--')\n","ax2.legend()\n","ax2.set_ylabel('Accuracy', color='r')\n","fig.tight_layout()\n","fig.savefig(os.path.join(path2write, 'alpha_test_lr_const_Batch_256Vs1024'))\n","fig.show()\n"],"metadata":{"id":"2w2HjZFHaa1C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = DNN()\n","model1 = model_lr1e2_B1024\n","model2 = model_lr1e4_B1024\n","alphaslr, train_losslr, train_accuracylr, val_losslr, val_accuracylr = alpha_test(model, model1, model2, training_DataLoader, validation_DataLoader, loss_fn)"],"metadata":{"id":"igM-qFeePj9c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax1 = plt.subplots(figsize=(15, 5))\n","\n","ax1 = fig.add_subplot(1,1,1)\n","ax1.plot(alphaslr, train_losslr, 'b-', label='Train')\n","ax1.plot(alphaslr, val_losslr, 'b--', label='Validation')\n","ax1.legend()\n","ax1.set_xlabel('Alpha')\n","ax1.set_ylabel('Loss', color='b')\n","ax1.set_title('Batch Size Constant; Learning Rate 1e-2 Vs 1e-4')\n","\n","ax2 = ax1.twinx()\n","ax2.plot(alphaslr, train_accuracylr, 'r-')\n","ax2.plot(alphaslr, val_accuracylr, 'r--')\n","ax2.legend()\n","ax2.set_ylabel('Accuracy', color='r')\n","fig.tight_layout()\n","fig.savefig(os.path.join(path2write, 'alpha_test_Batch_const_lr_1e-2Vs1e-4'))\n","fig.show()"],"metadata":{"id":"AgLlElOki3oe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"a6_RMTPfj3a-"},"execution_count":null,"outputs":[]}]}