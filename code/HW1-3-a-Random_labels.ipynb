{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true,"authorship_tag":"ABX9TyNQK0Db49DMZNTitCP8DNP0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"onYQI3ShIC2s"},"outputs":[],"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import sklearn\n","import seaborn\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import optim\n","import torch\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","from torchvision import transforms\n","\n","from torch.utils.data import TensorDataset\n","from tqdm import tqdm\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.utils.data import DataLoader\n","\n","import matplotlib.pyplot as plt\n","import random\n","from sklearn.decomposition import PCA"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"RyoNx19oVKR9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CNN2(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n","    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    self.dense1 = nn.Linear(32*14*14, 128)\n","    self.dense2 = nn.Linear(128, 10)\n","\n","  def forward(self, x):\n","    x = self.conv1(x)\n","    x = F.relu(x)\n","    x = self.pool1(x)\n","    x = x.view(x.shape[0], -1)\n","    x = self.dense1(x)\n","    x = self.dense2(x)\n","    out = F.log_softmax(x)\n","    return out"],"metadata":{"id":"vssR0yCdIKvi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Trainer:\n","\n","    def __init__(self,\n","                 model: torch.nn.Module,\n","                 device: torch.device,\n","                 criterion: torch.nn.Module,\n","                 optimizer: torch.optim.Optimizer,\n","                 training_DataLoader: torch.utils.data.Dataset,\n","                 validation_DataLoader: None,\n","                 # lr_scheduler: torch.optim.lr_scheduler = None,\n","                 epochs: int = 100,\n","                 epoch: int = 0,\n","                 notebook: bool = False,\n","                 path2write: str = None,\n","                 save_best=False,\n","                 save_final=True,\n","                 save_interval=10,\n","                 checkpoint_start_epoch=50\n","                 ):\n","        self.model = model\n","        self.criterion = criterion\n","        self.optimizer = optimizer\n","        # self.lr_scheduler = lr_scheduler\n","        self.training_DataLoader = training_DataLoader\n","        self.validation_DataLoader = validation_DataLoader\n","        self.device = device\n","        self.epochs = epochs\n","        self.epoch = epoch\n","        self.notebook = notebook\n","        #             if self.notebook:\n","        #                 print('Notebook')\n","        #                 from tqdm.notebook import tqdm, trange\n","        #             else:\n","        #                 from tqdm import tqdm, trange\n","        self.path2write = path2write\n","        LOG_DIR = os.path.join(path2write, 'Log')  # path2write + 'Log/'\n","        self.writer_train = SummaryWriter(os.path.join(LOG_DIR, \"train\"))\n","        self.writer_val = SummaryWriter(os.path.join(LOG_DIR, \"val\"))\n","        self.check_point_path = os.path.join(path2write, 'check_points')\n","        if not os.path.exists(self.check_point_path):\n","            os.makedirs(self.check_point_path)\n","        self.save_best = save_best\n","        self.save_final = save_final\n","        self.save_interval = save_interval\n","        self.checkpoint_start_epoch = checkpoint_start_epoch\n","        self.training_loss = []\n","        self.validation_loss = []\n","        self.learning_rate = []\n","        self.training_accuracy = []\n","        self.validation_accuracy = []\n","\n","    def run_trainer(self):\n","        self.model.to(self.device)\n","        #         print(next(self.model.parameters()).device)\n","        if self.notebook:\n","            print('Notebook')\n","            from tqdm.notebook import tqdm, trange\n","        else:\n","            from tqdm import tqdm, trange\n","        #         print(self.epochs)\n","        progressbar = trange(self.epochs, desc='Progress', disable=True)  # don't show progressbar\n","        loss_max = None\n","        for epoch in progressbar:\n","            print(f'Epoch - {epoch}')\n","\n","            # Training Block\n","            train_loss, train_accuracy = self._train()\n","            self.writer_train.add_scalar(\"Train Loss\", train_loss, epoch)\n","            self.writer_train.add_scalar(\"Train Accuracy\", train_accuracy, epoch)\n","\n","\n","            # Val Block\n","            val_loss, val_accuracy = self._validate()\n","            self.writer_val.add_scalar(\"Val Loss\", val_loss, epoch)\n","            self.writer_val.add_scalar(\"Val Accuracy\", val_accuracy, epoch)\n","\n","            # lr\n","            self.writer_train.add_scalar(\"Learning Rate\", self.optimizer.param_groups[0]['lr'], epoch)\n","\n","            print('Epoch - {} Train Loss - {:.6f} Val Loss - {:.6f} Train Accuracy - {:.6f} Val Accuracy - {:.6f}'.format(epoch, train_loss, val_loss, train_accuracy, val_accuracy))\n","            if self.save_final:\n","                if epoch == self.epochs-1:\n","                    model_name = 'epoch-{}-loss{:.6f}'.format(epoch, val_loss)\n","                    torch.save(self.model.state_dict(), os.path.join(self.check_point_path, model_name))\n","            loss_max = val_loss\n","\n","        return self.training_loss, self.validation_loss, self.model, self.training_accuracy, self.validation_accuracy\n","\n","    def _train(self):\n","\n","        self.model.train()\n","        train_losses = []\n","        batch_iter = tqdm(enumerate(self.training_DataLoader), 'Training', total=len(self.training_DataLoader),\n","                          disable=False)\n","        batch_acc = 0\n","        for i, (x, y) in batch_iter:\n","            input, target = x.type(torch.float32).to(self.device), y.type(torch.float32).to(self.device)\n","            self.optimizer.zero_grad()\n","            target = target.type(torch.LongTensor).to(self.device)\n","            output = self.model(input)\n","            loss = self.criterion(output, target)\n","            train_losses.append(loss.item())\n","            loss.backward()\n","            self.optimizer.step()\n","            pred = output.argmax(dim=1, keepdim=True) # max of prob\n","            pred = pred.flatten()\n","            batch_acc += torch.mean(pred.eq(target.view_as(pred)).type(torch.FloatTensor))\n","        accuracy = batch_acc/len(self.training_DataLoader)\n","        self.training_loss.append(np.mean(train_losses))  # Mean batch loss\n","        self.learning_rate.append(self.optimizer.param_groups[0]['lr'])\n","        self.training_accuracy.append(accuracy)\n","\n","        batch_iter.close()  # clean up the bar\n","        return np.mean(train_losses), accuracy\n","\n","    def _validate(self):\n","\n","        self.model.eval()\n","        valid_losses = []\n","        batch_iter = tqdm(enumerate(self.validation_DataLoader), 'validation', total=len(self.validation_DataLoader), disable=False)\n","        batch_acc = 0\n","        for i, (x, y) in batch_iter:\n","            input, target = x.type(torch.float32).to(self.device), y.to(self.device)\n","            with torch.no_grad():\n","                output = self.model(input)\n","                target = target.type(torch.LongTensor).to(self.device)\n","                loss = self.criterion(output, target)\n","                valid_losses.append(loss.item())\n","                pred = output.argmax(dim=1, keepdim=True)\n","                batch_acc += torch.mean(pred.eq(target.view_as(pred)).type(torch.FloatTensor)).item()\n","\n","        accuracy = batch_acc/len(self.validation_DataLoader)\n","        self.validation_loss.append(np.mean(valid_losses))\n","        self.validation_accuracy.append(accuracy)\n","        batch_iter.close()\n","        return np.mean(valid_losses), accuracy"],"metadata":{"id":"3psg3dl9KjU0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bath_size = 1020\n","\n","transform=transforms.Compose([\n","        transforms.ToTensor()\n","        ])\n","dataset1 = datasets.MNIST('../data', train=True, download=True, transform=transform)\n","random.shuffle(dataset1.targets)\n","\n","dataset2 = datasets.MNIST('../data', train=False, transform=transform)\n","\n","training_DataLoader = DataLoader(dataset1, batch_size=bath_size, shuffle=True)\n","validation_DataLoader= DataLoader(dataset2, batch_size=bath_size, shuffle=True)"],"metadata":{"id":"kfIYbiAwKylt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gpu_id = 0\n","loss_fn = nn.CrossEntropyLoss()\n","lr = 1e-4\n","epochs =  100\n","notebook = True\n","checkpoint_start_epoch = 5 #Not using\n","path2write = \"drive/MyDrive/DL_homework/HW1_1/CNN/\""],"metadata":{"id":"iVXWv9pkK1aE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = CNN2()\n","optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n","trainer = Trainer(model=model,\n","                      device=gpu_id,\n","                      criterion=loss_fn,\n","                      optimizer=optimizer,\n","                      training_DataLoader=training_DataLoader,\n","                      validation_DataLoader=validation_DataLoader,\n","                      # lr_scheduler=lr_scheduler,\n","                      epochs=epochs,\n","                      epoch=0,\n","                      notebook=True,\n","                      path2write= path2write,\n","                      checkpoint_start_epoch=checkpoint_start_epoch )\n","training_loss, validation_loss, model, training_accuracy, validation_accuracy = trainer.run_trainer()"],"metadata":{"id":"A5SSbijKLPdy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = plt.figure(figsize=(15,5))\n","\n","ax1 = fig.add_subplot(1,1,1)\n","ax1.plot(training_loss, 'r-', label='Taining Loss')\n","ax1.plot(validation_loss, 'b-', label='Validation Loss')\n","ax1.legend()\n","ax1.set_title('Loss')\n","\n","plt.show()\n","fig.savefig(os.path.join(path2write, 'Random Labels loss.png'))"],"metadata":{"id":"P9xzioNJL0yT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Ukm_ST1NbuWX"},"execution_count":null,"outputs":[]}]}