{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"rgcIUxl9ZZVM"},"outputs":[],"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import sklearn\n","import seaborn\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import optim\n","import torch\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","from torchvision import transforms\n","\n","from torch.utils.data import TensorDataset\n","from tqdm import tqdm\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.utils.data import DataLoader\n","\n","import matplotlib.pyplot as plt\n","import random"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"wAA1vKXv4Afb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Trainer:\n","\n","    def __init__(self,\n","                 model: torch.nn.Module,\n","                 device: torch.device,\n","                 criterion: torch.nn.Module,\n","                 optimizer: torch.optim.Optimizer,\n","                 training_DataLoader: torch.utils.data.Dataset,\n","                 validation_DataLoader: None,\n","                 # lr_scheduler: torch.optim.lr_scheduler = None,\n","                 epochs: int = 100,\n","                 epoch: int = 0,\n","                 notebook: bool = False,\n","                 path2write: str = None,\n","                 save_best=False,\n","                 save_final=True,\n","                 save_interval=10,\n","                 checkpoint_start_epoch=50,\n","                 gradient_norm = False\n","                 ):\n","        self.model = model\n","        self.criterion = criterion\n","        self.optimizer = optimizer\n","        # self.lr_scheduler = lr_scheduler\n","        self.training_DataLoader = training_DataLoader\n","        self.validation_DataLoader = validation_DataLoader\n","        self.device = device\n","        self.epochs = epochs\n","        self.epoch = epoch\n","        self.notebook = notebook\n","        self.path2write = path2write\n","        LOG_DIR = os.path.join(path2write, 'Log')  # path2write + 'Log/'\n","        self.writer_train = SummaryWriter(os.path.join(LOG_DIR, \"train\"))\n","        self.writer_val = SummaryWriter(os.path.join(LOG_DIR, \"val\"))\n","        self.check_point_path = os.path.join(path2write, 'check_points')\n","        if not os.path.exists(self.check_point_path):\n","            os.makedirs(self.check_point_path)\n","        self.save_best = save_best\n","        self.save_final = save_final\n","        self.save_interval = save_interval\n","        self.checkpoint_start_epoch = checkpoint_start_epoch\n","        self.training_loss = []\n","        self.validation_loss = []\n","        self.learning_rate = []\n","        self.gradient_norm = gradient_norm\n","        self.grad_list = []\n","\n","    def run_trainer(self):\n","        self.model.to(self.device)\n","        #         print(next(self.model.parameters()).device)\n","        if self.notebook:\n","            print('Notebook')\n","            from tqdm.notebook import tqdm, trange\n","        else:\n","            from tqdm import tqdm, trange\n","        #         print(self.epochs)\n","        progressbar = trange(self.epochs, desc='Progress', disable=True)  # don't show progressbar\n","        loss_max = None\n","        for epoch in progressbar:\n","            print(f'Epoch - {epoch}')\n","\n","            # Training Block\n","            train_loss = self._train()\n","            self.writer_train.add_scalar(\"Loss\", train_loss, epoch)\n","\n","            # Val Block\n","            val_loss = self._validate()\n","            self.writer_val.add_scalar(\"Loss\", val_loss, epoch)\n","\n","            # lr\n","            self.writer_train.add_scalar(\"Learning Rate\", self.optimizer.param_groups[0]['lr'], epoch)\n","\n","            # print('Epoch - {} Train Loss - {:.6f} Val Loss - {:.6f}'.format(epoch, train_loss, val_loss))\n","            if self.save_final:\n","                if epoch == self.epochs-1:\n","                    model_name = 'epoch-{}-loss{:.6f}'.format(epoch, val_loss)\n","                    torch.save(self.model.state_dict(), os.path.join(self.check_point_path, model_name))\n","\n","            loss_max = val_loss\n","\n","            if self.gradient_norm:\n","              grad_all = 0.0\n","              for p in self.model.parameters():\n","                grad = 0.0\n","                if p.grad is not None:\n","                  grad = (p.grad.cpu().data.numpy()**2).sum()\n","                  grad_all += grad\n","              self.grad_list.append(grad_all**0.5)\n","\n","        if self.gradient_norm:\n","          return self.grad_list, self.training_loss\n","        return self.training_loss, self.validation_loss, self.model\n","\n","    def _train(self):\n","\n","        self.model.train()\n","        train_losses = []\n","        batch_iter = tqdm(enumerate(self.training_DataLoader), 'Training', total=len(self.training_DataLoader),\n","                          disable=True)\n","        for i, (x, y) in batch_iter:\n","            input, target = x.type(torch.float32).to(self.device), y.type(torch.float32).to(self.device)\n","            self.optimizer.zero_grad()\n","            output = self.model(input)\n","\n","            loss = self.criterion(output, target)\n","            train_losses.append(loss.item())\n","            loss.backward()\n","            self.optimizer.step()\n","\n","        self.training_loss.append(np.mean(train_losses))  # Mean batch loss\n","        self.learning_rate.append(self.optimizer.param_groups[0]['lr'])\n","\n","        batch_iter.close()  # clean up the bar\n","        return np.mean(train_losses)\n","\n","    def _validate(self):\n","\n","        self.model.eval()\n","        valid_losses = []\n","        batch_iter = tqdm(enumerate(self.validation_DataLoader), 'validation', total=len(self.validation_DataLoader),\n","                          disable=True)\n","        for i, (x, y) in batch_iter:\n","            input, target = x.type(torch.float32).to(self.device), y.type(torch.float32).to(self.device)\n","            with torch.no_grad():\n","                out = self.model(input)\n","                loss = self.criterion(target, out)\n","                valid_losses.append(loss.item())\n","        self.validation_loss.append(np.mean(valid_losses))\n","        batch_iter.close()\n","        return np.mean(valid_losses)"],"metadata":{"id":"X6YqByzaZsyH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class model1(nn.Module):\n","    def __init__(self, input_size=1, output_size=1):\n","        super().__init__()\n","        self.dense1 = nn.Linear(input_size, 10)\n","        self.dense2 = nn.Linear(10, 18)\n","        self.dense3 = nn.Linear(18, 15)\n","        self.dense4 = nn.Linear(15, 4)\n","        self.dense5 = nn.Linear(4, output_size)\n","\n","    def forward(self, input_data):\n","        x1 = F.relu(self.dense1(input_data))\n","        x2 = F.relu(self.dense2(x1))\n","        x3 = F.relu(self.dense3(x2))\n","        x4 = F.relu(self.dense4(x3))\n","        x5 = F.relu(self.dense5(x4))\n","        return x5"],"metadata":{"id":"rPzNiITabvG9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prep_data(func, data_length=2500, train_ratio=0.7, batch_size=8, shuffle=True):\n","    X = np.linspace(1e-4, 1, data_length)\n","    # np.random.shuffle(X)\n","    y = np.array(list(map(func, X)))\n","    X = X.reshape(X.shape[0], 1)\n","    y = y.reshape(y.shape[0], 1)\n","    X = torch.from_numpy(X).float()\n","    y = torch.from_numpy(y).float()\n","    X_train, X_val = X[0:int(data_length * train_ratio), ], X[int(data_length * train_ratio):, ]\n","    y_train, y_val = y[0:int(data_length * train_ratio), ], y[int(data_length * train_ratio):, ]\n","    assert X_train.shape[0] == y_train.shape[0]\n","    assert X_val.shape[0] == y_val.shape[0]\n","    TrainDataLoader = DataLoader(TensorDataset(X_train, y_train), batch_size, shuffle)\n","    ValDataLoader = DataLoader(TensorDataset(X_val, y_val), batch_size, shuffle)\n","\n","    return TrainDataLoader, ValDataLoader"],"metadata":{"id":"5b-O25x3evBT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gpu_id = 0\n","loss_fn = nn.MSELoss()\n","lr = 1e-4\n","func1 = lambda x: (np.sin(5 * (np.pi) * x)) / (5 * np.pi * x)\n","func2 = lambda x: np.sign(np.sin(5*np.pi*x))\n","training_DataLoader,  validation_DataLoader = prep_data(func=func1,batch_size=4096)\n","epochs =  20000\n","notebook = True\n","checkpoint_start_epoch = 5 #Not using\n","path2write = \"drive/MyDrive/DL_homework/HW1_1/grad_norm/\""],"metadata":{"id":"BVjHRAJtbvwD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = model1()\n","optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n","trainer = Trainer(model=model,\n","                      device=gpu_id,\n","                      criterion=loss_fn,\n","                      optimizer=optimizer,\n","                      training_DataLoader=training_DataLoader,\n","                      validation_DataLoader=validation_DataLoader,\n","                      # lr_scheduler=lr_scheduler,\n","                      epochs=epochs,\n","                      epoch=0,\n","                      notebook=True,\n","                      path2write= path2write,\n","                      checkpoint_start_epoch=checkpoint_start_epoch,\n","                      gradient_norm = True)\n","grad_norm, training_loss = trainer.run_trainer()\n","\n","fig = plt.figure()\n","\n","ax1 = fig.add_subplot(1,2,1)\n","ax1.plot(training_loss, 'r-', label='Training Loss')\n","ax1.legend()\n","ax1.set_title('Training Loss')\n","fig.savefig(os.path.join(path2write, 'Training Loss.png'))\n","\n","ax2 = fig.add_subplot(1, 2, 2)\n","ax2.plot(grad_norm, 'b-', label='Gradient Norm')\n","ax2.legend()\n","ax2.set_title('Training Loss')\n","fig.savefig(os.path.join(path2write, 'Gradient Norm.png'))"],"metadata":{"id":"cdeBwGzeenVb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = plt.figure()\n","\n","ax1 = fig.add_subplot(1,2,1)\n","ax1.plot(training_loss, 'r-', label='Training Loss')\n","ax1.legend()\n","ax1.set_title('Training Loss')\n","fig.savefig(os.path.join(path2write, 'Training Loss.png'))\n","\n","ax2 = fig.add_subplot(1, 2, 2)\n","ax2.plot(grad_norm, 'b-', label='Gradient Norm')\n","ax2.legend()\n","ax2.set_title('Training Loss')\n","fig.savefig(os.path.join(path2write, 'Gradient Norm.png'))\n","\n","\n","plt.show()\n"],"metadata":{"id":"E41g6Tma8MJ2"},"execution_count":null,"outputs":[]}]}